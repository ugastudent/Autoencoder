{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAD8CAYAAADdVNcyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aa4d290978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This piece of software is bound by The MIT License (MIT)\n",
    "# Copyright (c) 2013 Siddharth Agrawal\n",
    "# Code written by : Siddharth Agrawal\n",
    "# Email ID : siddharth.950@gmail.com\n",
    "\n",
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import scipy.io\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" The Sparse Autoencoder class \"\"\"\n",
    "\n",
    "class SparseAutoencoder(object):\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Initialization of Autoencoder object \"\"\"\n",
    "\n",
    "    def __init__(self, visible_size, hidden_size, rho, lamda, beta):\n",
    "    \n",
    "        \"\"\" Initialize parameters of the Autoencoder object \"\"\"\n",
    "    \n",
    "        self.visible_size = visible_size    # number of input units\n",
    "        self.hidden_size = hidden_size      # number of hidden units\n",
    "        self.rho = rho                      # desired average activation of hidden units\n",
    "        self.lamda = lamda                  # weight decay parameter\n",
    "        self.beta = beta                    # weight of sparsity penalty term\n",
    "        \n",
    "        \"\"\" Set limits for accessing 'theta' values \"\"\"\n",
    "        \n",
    "        self.limit0 = 0\n",
    "        self.limit1 = hidden_size * visible_size\n",
    "        self.limit2 = 2 * hidden_size * visible_size\n",
    "        self.limit3 = 2 * hidden_size * visible_size + hidden_size\n",
    "        self.limit4 = 2 * hidden_size * visible_size + hidden_size + visible_size\n",
    "        \n",
    "        \"\"\" Initialize Neural Network weights randomly\n",
    "            W1, W2 values are chosen in the range [-r, r] \"\"\"\n",
    "        \n",
    "        r = math.sqrt(6) / math.sqrt(visible_size + hidden_size + 1)\n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time()))\n",
    "        \n",
    "        W1 = numpy.asarray(rand.uniform(low = -r, high = r, size = (hidden_size, visible_size)))\n",
    "        W2 = numpy.asarray(rand.uniform(low = -r, high = r, size = (visible_size, hidden_size)))\n",
    "        \n",
    "        \"\"\" Bias values are initialized to zero \"\"\"\n",
    "        \n",
    "        b1 = numpy.zeros((hidden_size, 1))\n",
    "        b2 = numpy.zeros((visible_size, 1))\n",
    "\n",
    "        \"\"\" Create 'theta' by unrolling W1, W2, b1, b2 \"\"\"\n",
    "\n",
    "        self.theta = numpy.concatenate((W1.flatten(), W2.flatten(),\n",
    "                                        b1.flatten(), b2.flatten()))\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Returns elementwise sigmoid output of input array \"\"\"\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "    \n",
    "        return (1 / (1 + numpy.exp(-x)))\n",
    "\n",
    "    #######################################################################################\n",
    "    \"\"\" Returns the cost of the Autoencoder and gradient at a particular 'theta' \"\"\"\n",
    "        \n",
    "    def sparseAutoencoderCost(self, theta, input):\n",
    "        \n",
    "        \"\"\" Extract weights and biases from 'theta' input \"\"\"\n",
    "        \n",
    "        W1 = theta[self.limit0 : self.limit1].reshape(self.hidden_size, self.visible_size)\n",
    "        W2 = theta[self.limit1 : self.limit2].reshape(self.visible_size, self.hidden_size)\n",
    "        b1 = theta[self.limit2 : self.limit3].reshape(self.hidden_size, 1)\n",
    "        b2 = theta[self.limit3 : self.limit4].reshape(self.visible_size, 1)\n",
    "        \n",
    "        \"\"\" Compute output layers by performing a feedforward pass\n",
    "            Computation is done for all the training inputs simultaneously \"\"\"\n",
    "        \n",
    "        hidden_layer = self.sigmoid(numpy.dot(W1, input) + b1)\n",
    "        output_layer = self.sigmoid(numpy.dot(W2, hidden_layer) + b2)\n",
    "        \n",
    "        \"\"\" Estimate the average activation value of the hidden layers \"\"\"\n",
    "        \n",
    "        rho_cap = numpy.sum(hidden_layer, axis = 1) / input.shape[1]\n",
    "        \n",
    "        \"\"\" Compute intermediate difference values using Backpropagation algorithm \"\"\"\n",
    "        \n",
    "        diff = output_layer - input\n",
    "        \n",
    "        sum_of_squares_error = 0.5 * numpy.sum(numpy.multiply(diff, diff)) / input.shape[1]\n",
    "        weight_decay         = 0.5 * self.lamda * (numpy.sum(numpy.multiply(W1, W1)) +\n",
    "                                                   numpy.sum(numpy.multiply(W2, W2)))\n",
    "        KL_divergence        = self.beta * numpy.sum(self.rho * numpy.log(self.rho / rho_cap) +\n",
    "                                                    (1 - self.rho) * numpy.log((1 - self.rho) / (1 - rho_cap)))\n",
    "        cost                 = sum_of_squares_error + weight_decay + KL_divergence\n",
    "        \n",
    "        KL_div_grad = self.beta * (-(self.rho / rho_cap) + ((1 - self.rho) / (1 - rho_cap)))\n",
    "        \n",
    "        del_out = numpy.multiply(diff, numpy.multiply(output_layer, 1 - output_layer))\n",
    "        del_hid = numpy.multiply(numpy.dot(numpy.transpose(W2), del_out) + numpy.transpose(numpy.matrix(KL_div_grad)), \n",
    "                                 numpy.multiply(hidden_layer, 1 - hidden_layer))\n",
    "        \n",
    "        \"\"\" Compute the gradient values by averaging partial derivatives\n",
    "            Partial derivatives are averaged over all training examples \"\"\"\n",
    "            \n",
    "        W1_grad = numpy.dot(del_hid, numpy.transpose(input))\n",
    "        W2_grad = numpy.dot(del_out, numpy.transpose(hidden_layer))\n",
    "        b1_grad = numpy.sum(del_hid, axis = 1)\n",
    "        b2_grad = numpy.sum(del_out, axis = 1)\n",
    "            \n",
    "        W1_grad = W1_grad / input.shape[1] + self.lamda * W1\n",
    "        W2_grad = W2_grad / input.shape[1] + self.lamda * W2\n",
    "        b1_grad = b1_grad / input.shape[1]\n",
    "        b2_grad = b2_grad / input.shape[1]\n",
    "        \n",
    "        \"\"\" Transform numpy matrices into arrays \"\"\"\n",
    "        \n",
    "        W1_grad = numpy.array(W1_grad)\n",
    "        W2_grad = numpy.array(W2_grad)\n",
    "        b1_grad = numpy.array(b1_grad)\n",
    "        b2_grad = numpy.array(b2_grad)\n",
    "        \n",
    "        \"\"\" Unroll the gradient values and return as 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = numpy.concatenate((W1_grad.flatten(), W2_grad.flatten(),\n",
    "                                        b1_grad.flatten(), b2_grad.flatten()))\n",
    "                                        \n",
    "        return [cost, theta_grad]\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Normalize the dataset provided as input \"\"\"\n",
    "\n",
    "def normalizeDataset(dataset):\n",
    "\n",
    "    \"\"\" Remove mean of dataset \"\"\"\n",
    "\n",
    "    dataset = dataset - numpy.mean(dataset)\n",
    "    \n",
    "    \"\"\" Truncate to +/-3 standard deviations and scale to -1 to 1 \"\"\"\n",
    "    \n",
    "    std_dev = 3 * numpy.std(dataset)\n",
    "    dataset = numpy.maximum(numpy.minimum(dataset, std_dev), -std_dev) / std_dev\n",
    "    \n",
    "    \"\"\" Rescale from [-1, 1] to [0.1, 0.9] \"\"\"\n",
    "    \n",
    "    dataset = (dataset + 1) * 0.4 + 0.1\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Randomly samples image patches, normalizes them and returns as dataset \"\"\"\n",
    "\n",
    "def loadDataset(num_patches, patch_side):\n",
    "\n",
    "    \"\"\" Load images into numpy array \"\"\"\n",
    "\n",
    "    images = scipy.io.loadmat('IMAGES.mat')\n",
    "    images = images['IMAGES']\n",
    "    \n",
    "    \"\"\" Initialize dataset as array of zeros \"\"\"\n",
    "    \n",
    "    dataset = numpy.zeros((patch_side*patch_side, num_patches))\n",
    "    \n",
    "    \"\"\" Initialize random numbers for random sampling of images\n",
    "        There are 10 images of size 512 X 512 \"\"\"\n",
    "    \n",
    "    rand = numpy.random.RandomState(int(time.time()))\n",
    "    image_indices = rand.randint(512 - patch_side, size = (num_patches, 2))\n",
    "    image_number  = rand.randint(10, size = num_patches)\n",
    "    \n",
    "    \"\"\" Sample 'num_patches' random image patches \"\"\"\n",
    "    \n",
    "    for i in range(num_patches):\n",
    "    \n",
    "        \"\"\" Initialize indices for patch extraction \"\"\"\n",
    "    \n",
    "        index1 = image_indices[i, 0]\n",
    "        index2 = image_indices[i, 1]\n",
    "        index3 = image_number[i]\n",
    "        \n",
    "        \"\"\" Extract patch and store it as a column \"\"\"\n",
    "        \n",
    "        patch = images[index1:index1+patch_side, index2:index2+patch_side, index3]\n",
    "        patch = patch.flatten()\n",
    "        dataset[:, i] = patch\n",
    "    \n",
    "    \"\"\" Normalize and return the dataset \"\"\"\n",
    "    \n",
    "    dataset = normalizeDataset(dataset)\n",
    "    return dataset\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Visualizes the obtained optimal W1 values as images \"\"\"\n",
    "\n",
    "def visualizeW1(opt_W1, vis_patch_side, hid_patch_side):\n",
    "\n",
    "    \"\"\" Add the weights as a matrix of images \"\"\"\n",
    "    \n",
    "    figure, axes = matplotlib.pyplot.subplots(nrows = hid_patch_side,\n",
    "                                              ncols = hid_patch_side)\n",
    "    index = 0\n",
    "                                              \n",
    "    for axis in axes.flat:\n",
    "    \n",
    "        \"\"\" Add row of weights as an image to the plot \"\"\"\n",
    "    \n",
    "        image = axis.imshow(opt_W1[index, :].reshape(vis_patch_side, vis_patch_side),\n",
    "                            cmap = matplotlib.pyplot.cm.gray, interpolation = 'nearest')\n",
    "        axis.set_frame_on(False)\n",
    "        axis.set_axis_off()\n",
    "        index += 1\n",
    "        \n",
    "    \"\"\" Show the obtained plot \"\"\"  \n",
    "        \n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "###########################################################################################\n",
    "\"\"\" Loads data, trains the Autoencoder and visualizes the learned weights \"\"\"\n",
    "\n",
    "def executeSparseAutoencoder():\n",
    "\n",
    "    \"\"\" Define the parameters of the Autoencoder \"\"\"\n",
    "    \n",
    "    vis_patch_side = 8      # side length of sampled image patches\n",
    "    hid_patch_side = 5      # side length of representative image patches\n",
    "    rho            = 0.01   # desired average activation of hidden units\n",
    "    lamda          = 0.0001 # weight decay parameter\n",
    "    beta           = 3      # weight of sparsity penalty term\n",
    "    num_patches    = 10000  # number of training examples\n",
    "    max_iterations = 400    # number of optimization iterations\n",
    "\n",
    "    visible_size = vis_patch_side * vis_patch_side  # number of input units\n",
    "    hidden_size  = hid_patch_side * hid_patch_side  # number of hidden units\n",
    "    \n",
    "    \"\"\" Load randomly sampled image patches as dataset \"\"\"\n",
    "    \n",
    "    training_data = loadDataset(num_patches, vis_patch_side)\n",
    "    \n",
    "    \"\"\" Initialize the Autoencoder with the above parameters \"\"\"\n",
    "    \n",
    "    encoder = SparseAutoencoder(visible_size, hidden_size, rho, lamda, beta)\n",
    "    \n",
    "    \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    \n",
    "    opt_solution  = scipy.optimize.minimize(encoder.sparseAutoencoderCost, encoder.theta, \n",
    "                                            args = (training_data,), method = 'L-BFGS-B', \n",
    "                                            jac = True, options = {'maxiter': max_iterations})\n",
    "    opt_theta     = opt_solution.x\n",
    "    opt_W1        = opt_theta[encoder.limit0 : encoder.limit1].reshape(hidden_size, visible_size)\n",
    "    \n",
    "    \"\"\" Visualize the obtained optimal W1 weights \"\"\"\n",
    "    \n",
    "    visualizeW1(opt_W1, vis_patch_side, hid_patch_side)\n",
    "\n",
    "executeSparseAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
